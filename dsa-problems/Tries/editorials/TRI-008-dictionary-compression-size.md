---
problem_id: TRI_DICTIONARY_COMPRESSION__2931
display_id: TRI-008
slug: dictionary-compression-size
title: "Dictionary Compression Size"
difficulty: Medium
difficulty_score: 45
topics:
  - Trie
  - String
  - Data Structures
tags:
  - trie
  - compression
  - memory-optimization
premium: true
subscription_tier: basic
---

# TRI-008: Dictionary Compression Size

## ğŸ“‹ Problem Summary

Given n words, compute the total number of nodes required in a trie to store them efficiently, including the root node.

## ğŸŒ Real-World Scenario

**Mobile Keyboard Dictionary Optimization**

Imagine you're an engineer at a mobile keyboard company like SwiftKey or Gboard. Your autocorrect dictionary contains hundreds of thousands of words, but mobile devices have limited memory. You need to determine how much memory your trie-based dictionary will consume before deploying it.

Consider storing these words: `["a", "ab", "abc"]`

**Naive Storage**: Store each word separately

- "a" â†’ 1 char
- "ab" â†’ 2 chars
- "abc" â†’ 3 chars
- **Total: 6 characters** in memory

**Trie Storage**: Share common prefixes

```
  root (1 node)
    â†“
   'a' (1 node, stores "a")
    â†“
   'b' (1 node, stores "ab")
    â†“
   'c' (1 node, stores "abc")
```

- **Total: 4 nodes** (root + a + b + c)
- Memory saved: 33% reduction!

**Why This Problem Matters:**

- **Memory Efficiency**: Mobile keyboards with 100K words can save 40-60% memory using tries
- **Performance Metrics**: Node count directly correlates with memory footprint and load time
- **Deployment Decisions**: Helps decide if trie-based storage fits within memory budgets
- **Cost Optimization**: Cloud services charge for memory usage; accurate estimation saves costs

**Industry Applications:**

1. **Mobile Keyboards**: Gboard, SwiftKey optimize dictionary storage for offline use
2. **Spell Checkers**: Microsoft Word, Google Docs compress dictionaries for fast loading
3. **URL Shorteners**: Bitly, TinyURL use tries to store and retrieve shortened URLs efficiently
4. **DNS Servers**: Cache domain names in memory-efficient trie structures
5. **Search Engines**: Index compression for billions of web pages

**Real Impact Example:**

A mobile keyboard with 150,000 English words:

- Raw storage: ~1.5 million characters (10 avg chars/word)
- Trie storage: ~400,000 nodes (with shared prefixes)
- **Memory saved: 73%** â†’ Fits in 2MB instead of 7.5MB!

![Real-World Application](../images/TRI-008/real-world-scenario.png)

## Detailed Explanation

The problem asks for a simple yet important metric: how many nodes does the trie contain?

**Key Insights:**

1. **Root node**: Every trie starts with one root node (counts as 1)
2. **Character nodes**: Each unique character in each position of any word requires a node
3. **Shared prefixes**: Words sharing prefixes share nodes, saving space
4. **Branching**: When words diverge, new branches are created

**Example Walkthrough:**

Words: `["a", "ab", "abc"]`

**Trie Node Creation:**

```
Build the trie step-by-step:

Step 1: Insert "a"
  Root
   |
   a (end) â† Node 2

  Nodes created: 2 (Root + a)

Step 2: Insert "ab"
  Root
   |
   a (end)  â† Reused!
   |
   b (end)  â† Node 3

  Nodes created: 3 (Root + a + b)
  Note: 'a' node reused, only 'b' is new

Step 3: Insert "abc"
  Root
   |
   a (end)  â† Reused!
   |
   b (end)  â† Reused!
   |
   c (end)  â† Node 4

  Nodes created: 4 (Root + a + b + c)
  Note: Both 'a' and 'b' reused, only 'c' is new

Comparison:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Without Trie (separate storage):
  "a"   â†’ 1 char
  "ab"  â†’ 2 chars
  "abc" â†’ 3 chars
  Total: 6 character positions

With Trie (shared prefixes):
  Total: 4 nodes (33% space saving!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Total nodes: 4
```

Build the trie:

```
Step 1: Insert "a"
  root
   |
   a (isEnd=true)
Nodes: 2 (root + a)

Step 2: Insert "ab"
  root
   |
   a (isEnd=true)
   |
   b (isEnd=true)
Nodes: 3 (root + a + b) - reused 'a'!

Step 3: Insert "abc"
  root
   |
   a (isEnd=true)
   |
   b (isEnd=true)
   |
   c (isEnd=true)
Nodes: 4 (root + a + b + c) - reused 'a' and 'b'!
```

**Total nodes: 4**

Without trie (separate storage): 1 + 2 + 3 = 6 character positions
With trie: 4 nodes (33% savings)

## Naive Approach

**Intuition:**

Count nodes by simulating trie construction with explicit tracking.

**Algorithm:**

1. Create a trie data structure with node creation tracking
2. Insert each word, creating new nodes as needed
3. Use a counter or unique node IDs to track total nodes created
4. Return the count

**Time Complexity:** O(N Ã— L) where N = number of words, L = average length
**Space Complexity:** O(N Ã— L) for the trie structure

**Why This Works:**

- Directly builds the trie and counts nodes
- Simple and straightforward
- Guaranteed correct result

**Limitations:**

- Not really "naive" - this IS the standard solution
- Could optimize by avoiding full trie construction if only count is needed
- For this problem, the direct approach is optimal

## Optimal Approach

**Key Insight:**

The "naive" approach is actually optimal for this problem. We must build the trie to count nodes accurately since node count depends on prefix sharing, which can only be determined during construction.

**Algorithm:**

1. **Initialize**: Create root node, set count = 1
2. **Insert Words**:
   ```
   for each word:
       node = root
       for each char in word:
           if char not in node.children:
               node.children[char] = new TrieNode()
               count++  // New node created
           node = node.children[char]
       mark node as end of word
   ```
3. **Return**: Total count

**Implementation Optimization:**

Instead of creating full trie objects, we can use a hash set to track unique paths, but this doesn't improve complexity.

**Time Complexity:** O(N Ã— L)

- Must process every character of every word exactly once
- Cannot be better than O(N Ã— L) since we must read all input

**Space Complexity:** O(N Ã— L) for the trie

- Worst case: no prefix sharing (all different first chars)
- Best case: O(L) if all words are prefixes of one long word

**Why This Is Optimal:**

- **Lower bound**: Must read all input â†’ Î©(N Ã— L)
- **Our solution**: O(N Ã— L) matches lower bound â†’ optimal
- **No redundant work**: Each character processed once
- **Efficient sharing**: Automatically handles prefix overlap

![Algorithm Visualization](../images/TRI-008/algorithm-visualization.png)

## Implementations

### Java


### Python


### C++


### JavaScript


### Common Mistakes to Avoid

1. **Forgetting the Root Node**

   - **Issue**: Not counting the root in the total
   - âŒ Wrong: Initialize count = 0
   - âœ… Correct: Initialize count = 1 (root exists)

2. **Double Counting Nodes**

   - **Issue**: Incrementing count even when reusing existing nodes
   - âŒ Wrong: Incrementing count for every character processed
   - âœ… Correct: Only increment when creating NEW nodes

3. **Confusing Node Count with Character Count**

   - **Issue**: Counting total characters instead of unique nodes
   - âŒ Wrong: Returning sum of all word lengths
   - âœ… Correct: Returning number of trie nodes (with sharing)

4. **Not Handling Empty Input**

   - **Issue**: Returning 0 for empty word list
   - âŒ Wrong: Return 0 when no words
   - âœ… Correct: Return 1 (root exists even with no words)

## Related Concepts

- **Trie Data Structure**: Fundamental prefix tree for string storage
- **Memory Optimization**: Efficient data structure usage
- **Prefix Compression**: Exploiting shared prefixes to save space
- **Dictionary Compression**: Real-world application in spell checkers and autocomplete


## Constraints

- `1 <= n <= 10^5` (number of words)
- Total character length across all words <= `2 Ã— 10^5`
- All words consist of lowercase English letters (a-z)
- Words may have duplicates